{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4563db88",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a simple yet powerful algorithm used for classification tasks in machine learning. It is based on Bayes' Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event. The \"naive\" aspect of the algorithm comes from the assumption that all features are independent of each other, which is often not the case in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bf9b6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ccdc4",
   "metadata": {},
   "source": [
    "Our dataset,\n",
    "\n",
    "| Outlook | Temperature | Humidity | Windy | Play |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| sunny | hot | high | false | no |\n",
    "| sunny | hot | high | true | no |\n",
    "| overcast | hot | high | false | yes |\n",
    "| rainy | mild | high | false | yes |\n",
    "| rainy | cool | normal | false | yes |\n",
    "| rainy | cool | normal | true | no |\n",
    "| overcast | cool | normal | true | yes |\n",
    "| sunny | mild | high | false | no |\n",
    "| sunny | cool | normal | false | yes |\n",
    "| rainy | mild | normal | false | yes |\n",
    "| sunny | mild | normal | true | yes |\n",
    "| overcast | mild | high | true | yes |\n",
    "| overcast | hot | normal | false | yes |\n",
    "| rainy | mild | high | true | no |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5f56b",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "###  Bayes' Theorem\n",
    "\n",
    "According to Bayes' theorem, this is proportional to the **prior** $P(y)$ multiplied by the **likelihood** $P(E | y)$:\n",
    "\n",
    "$$\n",
    "P(y | E) \\propto P(y) \\cdot P(E | y)\n",
    "$$\n",
    "\n",
    "The \"naive\" assumption is that all features are independent, so we can break down the likelihood:\n",
    "\n",
    "$$\n",
    "P(E | y) = P(\\text{sunny} | y) \\cdot P(\\text{cool} | y) \\cdot P(\\text{high} | y) \\cdot P(\\text{true} | y)\n",
    "$$\n",
    "\n",
    "This gives us the final formula we need to compare:\n",
    "\n",
    "$$\n",
    "Y_{predicted} = \\arg\\max_{y \\in \\{\\text{yes, no}\\}} \\left[ P(y) \\cdot P(\\text{sunny} | y) \\cdot P(\\text{cool} | y) \\cdot P(\\text{high} | y) \\cdot P(\\text{true} | y) \\right]\n",
    "$$\n",
    "\n",
    "### Likelihood Calculations (with Laplace Smoothing)\n",
    "\n",
    "Our code uses **Laplace (or Add-1) Smoothing** to prevent zero-probability problems.  The formula for each conditional probability is:\n",
    "\n",
    "$$\n",
    "P(x_i | y) = \\frac{\\text{count}(x_i, y) + 1}{\\text{count}(y) + k}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\text{count}(x_i, y)$ is the number of times the feature value $x_i$ appears with class $y$.\n",
    "* $\\text{count}(y)$ is the total number of times class $y$ appears.\n",
    "* $k$ is the total number of unique values for that feature (e.g., $k=3$ for Outlook, $k=2$ for Windy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f2ac1",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    ['sunny', 'hot', 'high', 'false', 'no'],\n",
    "    ['sunny', 'hot', 'high', 'true', 'no'],\n",
    "    ['overcast', 'hot', 'high', 'false', 'yes'],\n",
    "    ['rainy', 'mild', 'high', 'false', 'yes'],\n",
    "    ['rainy', 'cool', 'normal', 'false', 'yes'],\n",
    "    ['rainy', 'cool', 'normal', 'true', 'no'],\n",
    "    ['overcast', 'cool', 'normal', 'true', 'yes'],\n",
    "    ['sunny', 'mild', 'high', 'false', 'no'],\n",
    "    ['sunny', 'cool', 'normal', 'false', 'yes'],\n",
    "    ['rainy', 'mild', 'normal', 'false', 'yes'],\n",
    "    ['sunny', 'mild', 'normal', 'true', 'yes'],\n",
    "    ['overcast', 'mild', 'high', 'true', 'yes'],\n",
    "    ['overcast', 'hot', 'normal', 'false', 'yes'],\n",
    "    ['rainy', 'mild', 'high', 'true', 'no']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff9c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(data):\n",
    "    label_counts = {}\n",
    "    feature_counts = {}\n",
    "\n",
    "    for row in data:\n",
    "        outlook, temp, humidity, windy, label = row\n",
    "\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "        if label not in feature_counts:\n",
    "            feature_counts[label] = {\"Outlook\": {}, \"Temp\": {}, \"Humidity\": {}, \"Windy\": {}}\n",
    "\n",
    "        feature_counts[label][\"Outlook\"][outlook] = feature_counts[label][\"Outlook\"].get(outlook, 0) + 1\n",
    "        feature_counts[label][\"Temp\"][temp] = feature_counts[label][\"Temp\"].get(temp, 0) + 1\n",
    "        feature_counts[label][\"Humidity\"][humidity] = feature_counts[label][\"Humidity\"].get(humidity, 0) + 1\n",
    "        feature_counts[label][\"Windy\"][windy] = feature_counts[label][\"Windy\"].get(windy, 0) + 1\n",
    "\n",
    "    return label_counts, feature_counts\n",
    "\n",
    "label_counts, feature_counts = train_naive_bayes(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b64439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(x, label_counts, feature_counts):\n",
    "    total = sum(label_counts.values())\n",
    "    probs = {}\n",
    "    \n",
    "    feature_names = [\"Outlook\", \"Temp\", \"Humidity\", \"Windy\"]\n",
    "\n",
    "    for label in label_counts:\n",
    "        probs[label] = label_counts[label] / total\n",
    "\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            value = x[i] # Get 'sunny', then 'cool', etc.\n",
    "            \n",
    "            count = feature_counts[label][feature].get(value, 0)\n",
    "            \n",
    "            num_options = len(feature_counts[label][feature])\n",
    "\n",
    "            probs[label] *= (count + 1) / (label_counts[label] + num_options)\n",
    "\n",
    "    return max(probs, key=probs.get)\n",
    "\n",
    "\n",
    "test_sample = ['sunny', 'cool', 'high', 'true']\n",
    "prediction = predict_naive_bayes(test_sample, label_counts, feature_counts)\n",
    "\n",
    "print(\"Test Sample:\", test_sample)\n",
    "print(\"Predicted Class:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
